{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e74e8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "print(numpy.__version__)\n",
    "df = pd.read_csv(r'D:\\VJTI\\TY\\sem 6\\ds lab\\ds_project\\EV_Charging_Station_Optimization\\data\\processed\\final_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146e2e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        uid                    name             vendor_name  \\\n",
      "0  STATIC12  GensolCharge Pvt. Ltd.  GensolCharge Pvt. Ltd.   \n",
      "1  STATIC14                    REIL                    REIL   \n",
      "2  STATIC15                    REIL                    REIL   \n",
      "3  STATIC16                    REIL                    REIL   \n",
      "4  STATIC17                BluSmart                BluSmart   \n",
      "\n",
      "                                      address   latitude  longitude  \\\n",
      "0             NDSE Grid, BRPL South Extension  28.568238  77.219666   \n",
      "1                       Scada office kalka ji  28.541995  77.260583   \n",
      "2                   Ashram Chowk Mathura Road  28.571189  77.259806   \n",
      "3                  Nizamuddin Railway station  28.588991  77.253240   \n",
      "4  BSES Bhawan, Nehru Place, New Delhi 110048  28.549427  77.254636   \n",
      "\n",
      "        city country                 open                close  ...  \\\n",
      "0  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "1  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "2  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "3  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "4  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "\n",
      "            zone   0 available capacity cost_per_unit  power_type total  \\\n",
      "0  central-delhi NaN       NaN     15.0           NaN          DC   2.0   \n",
      "1  central-delhi NaN       NaN      3.3           NaN          AC   3.0   \n",
      "2  central-delhi NaN       NaN     15.0           NaN          DC   2.0   \n",
      "3  central-delhi NaN       NaN     15.0           NaN          DC   4.0   \n",
      "4  central-delhi NaN       NaN     15.0           NaN          DC   1.0   \n",
      "\n",
      "          type        vehicle_type         duration  \n",
      "0  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "1  BEVC AC 001  ['2W', '3W', '4W']  0 days 23:59:59  \n",
      "2  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "3  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "4  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "Number of rows: 2705\n"
     ]
    }
   ],
   "source": [
    "# If you loaded data from a CSV, check the initial load\n",
    "df = pd.read_csv(r\"D:\\VJTI\\TY\\sem 6\\ds lab\\ds_project\\EV_Charging_Station_Optimization\\data\\processed\\final_cleaned.csv\")\n",
    "print(df.head())  # Check the first few rows of the dataset\n",
    "print(f\"Number of rows: {len(df)}\")  # Check the number of rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db14912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'name', 'vendor_name', 'address', 'latitude', 'longitude',\n",
      "       'city', 'country', 'open', 'close', 'logo_url', 'staff',\n",
      "       'payment_modes', 'contact_numbers', 'station_type', 'postal_code',\n",
      "       'zone', '0', 'available', 'capacity', 'cost_per_unit', 'power_type',\n",
      "       'total', 'type', 'vehicle_type', 'duration'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68332da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'name', 'vendor_name', 'address', 'latitude', 'longitude',\n",
      "       'city', 'country', 'open', 'close', 'logo_url', 'staff',\n",
      "       'payment_modes', 'contact_numbers', 'station_type', 'postal_code',\n",
      "       'zone', '0', 'available', 'capacity', 'cost_per_unit', 'power_type',\n",
      "       'total', 'type', 'vehicle_type', 'duration', 'target'],\n",
      "      dtype='object')\n",
      "        uid                    name             vendor_name  \\\n",
      "0  STATIC12  GensolCharge Pvt. Ltd.  GensolCharge Pvt. Ltd.   \n",
      "1  STATIC14                    REIL                    REIL   \n",
      "2  STATIC15                    REIL                    REIL   \n",
      "3  STATIC16                    REIL                    REIL   \n",
      "4  STATIC17                BluSmart                BluSmart   \n",
      "\n",
      "                                      address   latitude  longitude  \\\n",
      "0             NDSE Grid, BRPL South Extension  28.568238  77.219666   \n",
      "1                       Scada office kalka ji  28.541995  77.260583   \n",
      "2                   Ashram Chowk Mathura Road  28.571189  77.259806   \n",
      "3                  Nizamuddin Railway station  28.588991  77.253240   \n",
      "4  BSES Bhawan, Nehru Place, New Delhi 110048  28.549427  77.254636   \n",
      "\n",
      "        city country                 open                close  ...   0  \\\n",
      "0  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ... NaN   \n",
      "1  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ... NaN   \n",
      "2  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ... NaN   \n",
      "3  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ... NaN   \n",
      "4  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ... NaN   \n",
      "\n",
      "  available capacity cost_per_unit power_type  total         type  \\\n",
      "0       NaN     15.0           NaN         DC    2.0  BEVC DC 001   \n",
      "1       NaN      3.3           NaN         AC    3.0  BEVC AC 001   \n",
      "2       NaN     15.0           NaN         DC    2.0  BEVC DC 001   \n",
      "3       NaN     15.0           NaN         DC    4.0  BEVC DC 001   \n",
      "4       NaN     15.0           NaN         DC    1.0  BEVC DC 001   \n",
      "\n",
      "         vehicle_type         duration  target  \n",
      "0              ['4W']  0 days 23:59:59       0  \n",
      "1  ['2W', '3W', '4W']  0 days 23:59:59       0  \n",
      "2              ['4W']  0 days 23:59:59       0  \n",
      "3              ['4W']  0 days 23:59:59       0  \n",
      "4              ['4W']  0 days 23:59:59       0  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "target\n",
      "1    1867\n",
      "0     838\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create the target column based on the rule\n",
    "df1 = df.copy()  # Create a new DataFrame (df1) from the original df\n",
    "df1['target'] = df1.apply(lambda row: 1 if (row['available'] < 2 and row['capacity'] < 5) else 0, axis=1)\n",
    "\n",
    "# Confirm that the 'target' column was added to df1\n",
    "print(df1.columns)  # List all column names to verify\n",
    "print(df1.head())   # View the first few rows of the DataFrame\n",
    "print(df1['target'].value_counts())  # Check the distribution of the 'target' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f7b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                   0\n",
      "name                  0\n",
      "vendor_name           0\n",
      "address               0\n",
      "latitude              0\n",
      "longitude             0\n",
      "city                  0\n",
      "country               0\n",
      "open                  0\n",
      "close                 0\n",
      "logo_url            467\n",
      "staff                 0\n",
      "payment_modes         0\n",
      "contact_numbers       0\n",
      "station_type          0\n",
      "postal_code           0\n",
      "zone                295\n",
      "0                  2705\n",
      "available           238\n",
      "capacity              3\n",
      "cost_per_unit       252\n",
      "power_type          208\n",
      "total               208\n",
      "type                208\n",
      "vehicle_type        208\n",
      "duration              0\n",
      "target                0\n",
      "dtype: int64\n",
      "target\n",
      "1    1867\n",
      "0     838\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any NaN or missing values\n",
    "print(df1.isna().sum())\n",
    "\n",
    "# Make sure no error occurs while creating the target\n",
    "print(df1['target'].value_counts())  # Confirm target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d50bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in original df: 2705\n",
      "        uid                    name             vendor_name  \\\n",
      "0  STATIC12  GensolCharge Pvt. Ltd.  GensolCharge Pvt. Ltd.   \n",
      "1  STATIC14                    REIL                    REIL   \n",
      "2  STATIC15                    REIL                    REIL   \n",
      "3  STATIC16                    REIL                    REIL   \n",
      "4  STATIC17                BluSmart                BluSmart   \n",
      "\n",
      "                                      address   latitude  longitude  \\\n",
      "0             NDSE Grid, BRPL South Extension  28.568238  77.219666   \n",
      "1                       Scada office kalka ji  28.541995  77.260583   \n",
      "2                   Ashram Chowk Mathura Road  28.571189  77.259806   \n",
      "3                  Nizamuddin Railway station  28.588991  77.253240   \n",
      "4  BSES Bhawan, Nehru Place, New Delhi 110048  28.549427  77.254636   \n",
      "\n",
      "        city country                 open                close  ...  \\\n",
      "0  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "1  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "2  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "3  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "4  New Delhi   India  2025-04-27 00:00:00  2025-04-27 23:59:59  ...   \n",
      "\n",
      "            zone   0 available capacity cost_per_unit  power_type total  \\\n",
      "0  central-delhi NaN       NaN     15.0           NaN          DC   2.0   \n",
      "1  central-delhi NaN       NaN      3.3           NaN          AC   3.0   \n",
      "2  central-delhi NaN       NaN     15.0           NaN          DC   2.0   \n",
      "3  central-delhi NaN       NaN     15.0           NaN          DC   4.0   \n",
      "4  central-delhi NaN       NaN     15.0           NaN          DC   1.0   \n",
      "\n",
      "          type        vehicle_type         duration  \n",
      "0  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "1  BEVC AC 001  ['2W', '3W', '4W']  0 days 23:59:59  \n",
      "2  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "3  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "4  BEVC DC 001              ['4W']  0 days 23:59:59  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in original df: {len(df)}\")\n",
    "print(df.head())  # View the first few rows of df to confirm it has data\n",
    "\n",
    "# If the training set is still empty, try using a different split or increase the size of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06d529e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after filling:\n",
      "uid                   0\n",
      "name                  0\n",
      "vendor_name           0\n",
      "address               0\n",
      "latitude              0\n",
      "longitude             0\n",
      "city                  0\n",
      "country               0\n",
      "open                  0\n",
      "close                 0\n",
      "logo_url              0\n",
      "staff                 0\n",
      "payment_modes         0\n",
      "contact_numbers       0\n",
      "station_type          0\n",
      "postal_code           0\n",
      "zone                  0\n",
      "0                  2705\n",
      "available             0\n",
      "capacity              0\n",
      "cost_per_unit         0\n",
      "power_type            0\n",
      "total                 0\n",
      "type                  0\n",
      "vehicle_type          0\n",
      "duration              0\n",
      "target                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Separate numeric and categorical columns\n",
    "numeric_cols = df1.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = df1.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Step 2: Fill missing values\n",
    "# Numeric columns: Fill with median (or mean if you prefer)\n",
    "df1[numeric_cols] = df1[numeric_cols].fillna(df1[numeric_cols].median())\n",
    "\n",
    "# Categorical columns: Fill with mode (most frequent value)\n",
    "for col in categorical_cols:\n",
    "    mode_val = df1[col].mode()[0]  # Find the most frequent value\n",
    "    df1[col] = df1[col].fillna(mode_val)\n",
    "\n",
    "# Step 3: Confirm if any missing values are left\n",
    "print(\"Missing values after filling:\")\n",
    "print(df1.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "524e7a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'name', 'vendor_name', 'address', 'latitude', 'longitude',\n",
      "       'city', 'country', 'open', 'close', 'logo_url', 'staff',\n",
      "       'payment_modes', 'contact_numbers', 'station_type', 'postal_code',\n",
      "       'zone', 'available', 'capacity', 'cost_per_unit', 'power_type', 'total',\n",
      "       'type', 'vehicle_type', 'duration', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop the '0' column from df1\n",
    "df1 = df1.drop(columns=['0'], errors='ignore')\n",
    "\n",
    "# Confirm it's gone\n",
    "print(df1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278256f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['latitude', 'longitude', 'staff', 'station_type', 'postal_code', 'zone',\n",
      "       'available', 'capacity', 'cost_per_unit', 'power_type', 'total', 'type',\n",
      "       'vehicle_type', 'duration', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Columns to drop (long text, IDs)\n",
    "cols_to_drop = ['uid', 'name', 'vendor_name', 'address', 'city', 'country', 'logo_url', 'payment_modes', 'contact_numbers', 'open', 'close']\n",
    "df1_model = df1.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Columns to encode (categorical to numeric)\n",
    "cols_to_encode = ['power_type', 'type', 'vehicle_type', 'zone', 'station_type']\n",
    "\n",
    "# Apply Label Encoding\n",
    "le = LabelEncoder()\n",
    "for col in cols_to_encode:\n",
    "    df1_model[col] = le.fit_transform(df1_model[col])\n",
    "\n",
    "# Confirm what columns are left\n",
    "print(df1_model.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b9ad3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid                 object\n",
      "name                object\n",
      "vendor_name         object\n",
      "address             object\n",
      "latitude           float64\n",
      "longitude          float64\n",
      "city                object\n",
      "country             object\n",
      "open                object\n",
      "close               object\n",
      "logo_url            object\n",
      "staff               object\n",
      "payment_modes       object\n",
      "contact_numbers     object\n",
      "station_type        object\n",
      "postal_code          int64\n",
      "zone                object\n",
      "0                  float64\n",
      "available          float64\n",
      "capacity           float64\n",
      "cost_per_unit       object\n",
      "power_type          object\n",
      "total              float64\n",
      "type                object\n",
      "vehicle_type        object\n",
      "duration            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)  # Display data types of all columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de665b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['uid', 'name', 'vendor_name', 'address', 'latitude', 'longitude',\n",
      "       'city', 'country', 'open', 'close', 'logo_url', 'staff',\n",
      "       'payment_modes', 'contact_numbers', 'station_type', 'postal_code',\n",
      "       'zone', '0', 'available', 'capacity', 'cost_per_unit', 'power_type',\n",
      "       'total', 'type', 'vehicle_type', 'duration'],\n",
      "      dtype='object')\n",
      "    latitude  longitude                 open                close  \\\n",
      "0  28.568238  77.219666  2025-04-27 00:00:00  2025-04-27 23:59:59   \n",
      "1  28.541995  77.260583  2025-04-27 00:00:00  2025-04-27 23:59:59   \n",
      "2  28.571189  77.259806  2025-04-27 00:00:00  2025-04-27 23:59:59   \n",
      "3  28.588991  77.253240  2025-04-27 00:00:00  2025-04-27 23:59:59   \n",
      "4  28.549427  77.254636  2025-04-27 00:00:00  2025-04-27 23:59:59   \n",
      "\n",
      "         payment_modes  station_type   0  available  capacity cost_per_unit  \\\n",
      "0  Card, E-Wallet, UPI             1 NaN        NaN      15.0           NaN   \n",
      "1             E-Wallet             1 NaN        NaN       3.3           NaN   \n",
      "2             E-Wallet             1 NaN        NaN      15.0           NaN   \n",
      "3        Cash/E-Wallet             1 NaN        NaN      15.0           NaN   \n",
      "4        Cash/E-Wallet             1 NaN        NaN      15.0           NaN   \n",
      "\n",
      "   power_type  total  type  vehicle_type         duration  \n",
      "0           1    2.0     4             2  0 days 23:59:59  \n",
      "1           0    3.0     3             0  0 days 23:59:59  \n",
      "2           1    2.0     4             2  0 days 23:59:59  \n",
      "3           1    4.0     4             2  0 days 23:59:59  \n",
      "4           1    1.0     4             2  0 days 23:59:59  \n",
      "latitude         float64\n",
      "longitude        float64\n",
      "open              object\n",
      "close             object\n",
      "payment_modes     object\n",
      "station_type       int32\n",
      "0                float64\n",
      "available        float64\n",
      "capacity         float64\n",
      "cost_per_unit     object\n",
      "power_type         int32\n",
      "total            float64\n",
      "type               int32\n",
      "vehicle_type       int32\n",
      "duration          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Print the actual column names to check if they match\n",
    "print(df.columns)\n",
    "\n",
    "# Drop irrelevant columns, use correct names after checking\n",
    "columns_to_drop = ['uid', 'name', 'vendor_name', 'address', 'city', 'country', 'logo_url', \n",
    "                  'staff', 'contact_numbers', 'postal_code', 'zone']\n",
    "\n",
    "# Check if those columns exist in the dataset before dropping\n",
    "columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Convert categorical columns to strings (in case of mixed data types)\n",
    "label_columns = ['station_type', 'power_type', 'type', 'vehicle_type']\n",
    "\n",
    "# Handle missing values and convert all to strings\n",
    "for col in label_columns:\n",
    "    df[col] = df[col].astype(str)  # Convert everything to string\n",
    "    df[col] = df[col].fillna('missing')  # Replace NaNs with a placeholder (e.g., 'missing')\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "le = LabelEncoder()\n",
    "for col in label_columns:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Check the first few rows of the cleaned data\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e672c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the 'cost_per_unit' column\n",
    "def clean_cost(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('₹', '').replace('per unit', '').strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0  # If still invalid, set to 0\n",
    "\n",
    "df1_model['cost_per_unit'] = df1_model['cost_per_unit'].apply(clean_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "624c637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data ready for modeling. Columns: ['latitude', 'longitude', 'staff', 'station_type', 'postal_code', 'zone', 'available', 'capacity', 'cost_per_unit', 'power_type', 'total', 'type', 'vehicle_type', 'duration', 'target']\n",
      "\n",
      "🎯 Model Performance:\n",
      "Mean Squared Error (MSE): 0.0021\n",
      "R-squared Score (R²): 0.9898\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Drop unnecessary text-heavy columns\n",
    "cols_to_drop = ['uid', 'name', 'vendor_name', 'address', 'city', 'country',\n",
    "                'open', 'close', 'logo_url', 'payment_modes', 'contact_numbers']\n",
    "df1_model = df1.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Step 2: Encode categorical columns (including 'staff')\n",
    "cols_to_encode = ['power_type', 'type', 'vehicle_type', 'zone', 'station_type', 'staff']\n",
    "le = LabelEncoder()\n",
    "for col in cols_to_encode:\n",
    "    df1_model[col] = le.fit_transform(df1_model[col])\n",
    "\n",
    "# Step 3: Clean 'cost_per_unit' column\n",
    "def clean_cost(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('₹', '').replace('per unit', '').strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0  # If still invalid, set to 0\n",
    "\n",
    "df1_model['cost_per_unit'] = df1_model['cost_per_unit'].apply(clean_cost)\n",
    "\n",
    "# Step 4: Clean 'duration' column (convert timedelta to total seconds)\n",
    "def clean_duration(x):\n",
    "    if isinstance(x, str) and 'days' in x:\n",
    "        # Convert '0 days 00:00:00' to seconds\n",
    "        return pd.to_timedelta(x).total_seconds()\n",
    "    return 0.0  # If invalid, set to 0\n",
    "\n",
    "df1_model['duration'] = df1_model['duration'].apply(clean_duration)\n",
    "\n",
    "# Step 5: Confirm data is clean\n",
    "print(\"✅ Data ready for modeling. Columns:\", df1_model.columns.tolist())\n",
    "\n",
    "# Step 6: Define X (features) and y (target)\n",
    "X = df1_model.drop(columns=['target'])  # Features\n",
    "y = df1_model['target']                 # Target variable\n",
    "\n",
    "# Step 7: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 8: Train Linear Regression Model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predict on Test Set\n",
    "y_pred = lin_reg.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n🎯 Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"R-squared Score (R²): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "070614fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data ready for Random Forest. Columns: ['latitude', 'longitude', 'staff', 'station_type', 'postal_code', 'zone', 'available', 'capacity', 'cost_per_unit', 'power_type', 'total', 'type', 'vehicle_type', 'duration', 'target']\n",
      "\n",
      "🎯 Random Forest Model Performance:\n",
      "Mean Squared Error (MSE): 0.0000\n",
      "R-squared Score (R²): 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Drop unnecessary text-heavy columns\n",
    "cols_to_drop = ['uid', 'name', 'vendor_name', 'address', 'city', 'country',\n",
    "                'open', 'close', 'logo_url', 'payment_modes', 'contact_numbers']\n",
    "df1_model = df1.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Step 2: Encode categorical columns (including 'staff')\n",
    "cols_to_encode = ['power_type', 'type', 'vehicle_type', 'zone', 'station_type', 'staff']\n",
    "le = LabelEncoder()\n",
    "for col in cols_to_encode:\n",
    "    df1_model[col] = le.fit_transform(df1_model[col])\n",
    "\n",
    "# Step 3: Clean 'cost_per_unit' column\n",
    "def clean_cost(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('₹', '').replace('per unit', '').strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0  # If still invalid, set to 0\n",
    "\n",
    "df1_model['cost_per_unit'] = df1_model['cost_per_unit'].apply(clean_cost)\n",
    "\n",
    "# Step 4: Clean 'duration' column (convert timedelta to total seconds)\n",
    "def clean_duration(x):\n",
    "    if isinstance(x, str) and 'days' in x:\n",
    "        # Convert '0 days 00:00:00' to seconds\n",
    "        return pd.to_timedelta(x).total_seconds()\n",
    "    return 0.0  # If invalid, set to 0\n",
    "\n",
    "df1_model['duration'] = df1_model['duration'].apply(clean_duration)\n",
    "\n",
    "# Step 5: Confirm data is clean\n",
    "print(\"✅ Data ready for Random Forest. Columns:\", df1_model.columns.tolist())\n",
    "\n",
    "# Step 6: Define X (features) and y (target)\n",
    "X = df1_model.drop(columns=['target'])  # Features\n",
    "y = df1_model['target']                 # Target variable\n",
    "\n",
    "# Step 7: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 8: Train Random Forest Model\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predict on Test Set\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"\\n🎯 Random Forest Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_rf:.4f}\")\n",
    "print(f\"R-squared Score (R²): {r2_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05d41f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data ready for XGBoost. Columns: ['latitude', 'longitude', 'staff', 'station_type', 'postal_code', 'zone', 'available', 'capacity', 'cost_per_unit', 'power_type', 'total', 'type', 'vehicle_type', 'duration', 'target']\n",
      "\n",
      "🎯 XGBoost Model Performance:\n",
      "Mean Squared Error (MSE): 0.0000\n",
      "R-squared Score (R²): 1.0000\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Drop unnecessary text-heavy columns\n",
    "cols_to_drop = ['uid', 'name', 'vendor_name', 'address', 'city', 'country',\n",
    "                'open', 'close', 'logo_url', 'payment_modes', 'contact_numbers']\n",
    "df1_model = df1.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Step 2: Encode categorical columns (including 'staff')\n",
    "cols_to_encode = ['power_type', 'type', 'vehicle_type', 'zone', 'station_type', 'staff']\n",
    "le = LabelEncoder()\n",
    "for col in cols_to_encode:\n",
    "    df1_model[col] = le.fit_transform(df1_model[col])\n",
    "\n",
    "# Step 3: Clean 'cost_per_unit' column\n",
    "def clean_cost(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('₹', '').replace('per unit', '').strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0  # If still invalid, set to 0\n",
    "\n",
    "df1_model['cost_per_unit'] = df1_model['cost_per_unit'].apply(clean_cost)\n",
    "\n",
    "# Step 4: Clean 'duration' column (convert timedelta to total seconds)\n",
    "def clean_duration(x):\n",
    "    if isinstance(x, str) and 'days' in x:\n",
    "        # Convert '0 days 00:00:00' to seconds\n",
    "        return pd.to_timedelta(x).total_seconds()\n",
    "    return 0.0  # If invalid, set to 0\n",
    "\n",
    "df1_model['duration'] = df1_model['duration'].apply(clean_duration)\n",
    "\n",
    "# Step 5: Confirm data is clean\n",
    "print(\"✅ Data ready for XGBoost. Columns:\", df1_model.columns.tolist())\n",
    "\n",
    "# Step 6: Define X (features) and y (target)\n",
    "X = df1_model.drop(columns=['target'])  # Features\n",
    "y = df1_model['target']                 # Target variable\n",
    "\n",
    "# Step 7: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 8: Train XGBoost Model\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predict on Test Set\n",
    "y_pred_xg = xg_reg.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "mse_xg = mean_squared_error(y_test, y_pred_xg)\n",
    "r2_xg = r2_score(y_test, y_pred_xg)\n",
    "\n",
    "print(\"\\n🎯 XGBoost Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_xg:.4f}\")\n",
    "print(f\"R-squared Score (R²): {r2_xg:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d934466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data ready for KNN. Columns: ['latitude', 'longitude', 'staff', 'station_type', 'postal_code', 'zone', 'available', 'capacity', 'cost_per_unit', 'power_type', 'total', 'type', 'vehicle_type', 'duration', 'target']\n",
      "\n",
      "🎯 KNN Model Performance:\n",
      "Mean Squared Error (MSE): 0.0074\n",
      "R-squared Score (R²): 0.9646\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Drop unnecessary text-heavy columns\n",
    "cols_to_drop = ['uid', 'name', 'vendor_name', 'address', 'city', 'country',\n",
    "                'open', 'close', 'logo_url', 'payment_modes', 'contact_numbers']\n",
    "df1_model = df1.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Step 2: Encode categorical columns (including 'staff')\n",
    "cols_to_encode = ['power_type', 'type', 'vehicle_type', 'zone', 'station_type', 'staff']\n",
    "le = LabelEncoder()\n",
    "for col in cols_to_encode:\n",
    "    df1_model[col] = le.fit_transform(df1_model[col])\n",
    "\n",
    "# Step 3: Clean 'cost_per_unit' column\n",
    "def clean_cost(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.replace('₹', '').replace('per unit', '').strip()\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0  # If still invalid, set to 0\n",
    "\n",
    "df1_model['cost_per_unit'] = df1_model['cost_per_unit'].apply(clean_cost)\n",
    "\n",
    "# Step 4: Clean 'duration' column (convert timedelta to total seconds)\n",
    "def clean_duration(x):\n",
    "    if isinstance(x, str) and 'days' in x:\n",
    "        # Convert '0 days 00:00:00' to seconds\n",
    "        return pd.to_timedelta(x).total_seconds()\n",
    "    return 0.0  # If invalid, set to 0\n",
    "\n",
    "df1_model['duration'] = df1_model['duration'].apply(clean_duration)\n",
    "\n",
    "# Step 5: Confirm data is clean\n",
    "print(\"✅ Data ready for KNN. Columns:\", df1_model.columns.tolist())\n",
    "\n",
    "# Step 6: Define X (features) and y (target)\n",
    "X = df1_model.drop(columns=['target'])  # Features\n",
    "y = df1_model['target']                 # Target variable\n",
    "\n",
    "# Step 7: Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 8: Train KNN Model\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)  # You can change n_neighbors to tune\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Predict on Test Set\n",
    "y_pred_knn = knn_reg.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(\"\\n🎯 KNN Model Performance:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_knn:.4f}\")\n",
    "print(f\"R-squared Score (R²): {r2_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82442c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR - MSE: 0.0572, R²: 0.7258\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "svr = SVR(kernel='rbf', C=1, epsilon=0.1)\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_svr = svr.predict(X_test)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f\"SVR - MSE: {mse_svr:.4f}, R²: {r2_svr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c870f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - MSE: 0.0000, R²: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_dt = dt_reg.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"Decision Tree - MSE: {mse_dt:.4f}, R²: {r2_dt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f4d797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost - MSE: 0.0000, R²: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "ada_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
    "ada_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_ada = ada_reg.predict(X_test)\n",
    "mse_ada = mean_squared_error(y_test, y_pred_ada)\n",
    "r2_ada = r2_score(y_test, y_pred_ada)\n",
    "\n",
    "print(f\"AdaBoost - MSE: {mse_ada:.4f}, R²: {r2_ada:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbc99c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting - MSE: 0.0000, R²: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_gb = gb_reg.predict(X_test)\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"Gradient Boosting - MSE: {mse_gb:.4f}, R²: {r2_gb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e631d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet - MSE: 0.0599, R²: 0.7128\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "en_reg = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42)\n",
    "en_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_en = en_reg.predict(X_test)\n",
    "mse_en = mean_squared_error(y_test, y_pred_en)\n",
    "r2_en = r2_score(y_test, y_pred_en)\n",
    "\n",
    "print(f\"ElasticNet - MSE: {mse_en:.4f}, R²: {r2_en:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2b31331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression - MSE: 0.0049, R²: 0.9764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "lasso_reg = Lasso(alpha=0.01)\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_lasso = lasso_reg.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\"Lasso Regression - MSE: {mse_lasso:.4f}, R²: {r2_lasso:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7374722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - MSE: 0.0021, R²: 0.9898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define and train the model\n",
    "ridge_reg = Ridge(alpha=1.0)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_ridge = ridge_reg.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Regression - MSE: {mse_ridge:.4f}, R²: {r2_ridge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4528c0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Linear Regression - MSE: 0.0000, R²: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=2)  # You can adjust the degree\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# Define and train the model\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_poly, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "X_test_poly = poly.transform(X_test)\n",
    "y_pred_poly = poly_reg.predict(X_test_poly)\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"Polynomial Linear Regression - MSE: {mse_poly:.4f}, R²: {r2_poly:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0a3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
